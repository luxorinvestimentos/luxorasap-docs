# ğŸ— Arquitetura â€” Luxor Data Pipelines

O **Luxor Data Pipelines** foi projetado para rodar **100% no Azure** utilizando **Azure Functions** como mecanismo de execuÃ§Ã£o.  
A arquitetura foi pensada para garantir **modularidade, escalabilidade e facilidade de manutenÃ§Ã£o** dos pipelines de dados.

---

## ğŸ“‚ OrganizaÃ§Ã£o de DiretÃ³rios

A estrutura Ã© dividida em **duas partes principais**:

1. **`pipelines/`**  
    - ContÃ©m a **lÃ³gica principal** de cada processo de ETL.  
    - Cada arquivo representa um pipeline independente.  
    - Cada script deve expor um mÃ©todo **`run()`** que recebe parÃ¢metros opcionais para controlar a execuÃ§Ã£o.

2. **DiretÃ³rios `run_[NOME_DO_PIPELINE]/`**  
    - Cada diretÃ³rio corresponde a uma **Azure Function** individual.  
    - ResponsÃ¡vel por **orquestrar** a execuÃ§Ã£o do pipeline definido em `pipelines/`.  
    - ContÃ©m:
        - `functions.json` â†’ ConfiguraÃ§Ã£o da Azure Function (tipo de trigger, agendamento, bindings, etc.).
        - `__init__.py` â†’ Ponto de entrada da Azure Function. Importa a DAG (pipeline) e executa via `main()`.
        - `[nome_do_pipeline].py` â†’ Define a **DAG** do pipeline, importando funÃ§Ãµes/mÃ³dulos de `pipelines/` e compondo a execuÃ§Ã£o.

---

## ğŸ”„ Fluxo de ExecuÃ§Ã£o

<div alingn="center">
```mermaid
flowchart LR
    subgraph "Azure Function"
        direction LR
        A[Trigger: Timer/HTTP] --> B["main() no '__init__.py'"]
        B --> C["FunÃ§Ã£o run() em run_[pipeline].py"]
        C --> D[ExecuÃ§Ã£o de scripts no pipelines/]
    end
```
</div>

1. O **gatilho** (Timer Trigger ou outro) dispara a execuÃ§Ã£o.
2. O **`main()`** do `__init__.py` chama o **`run()`** da DAG definida no arquivo `.py` do diretÃ³rio `run_...`.
3. O **`run()`** da DAG importa e executa as funÃ§Ãµes `run()` dos mÃ³dulos em `pipelines/`.
4. O processamento Ã© feito (extraÃ§Ã£o, transformaÃ§Ã£o, carga) e os resultados sÃ£o salvos no ADLS.

---

## ğŸ“Œ Exemplo de Estrutura

```
luxor-data-pipelines/
â”‚
â”œâ”€â”€ pipelines/
â”‚   â””â”€â”€ hist_returns.py
â”‚
â””â”€â”€ run_hist_returns/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ functions.json
    â””â”€â”€ run_hist_returns.py
```

---

### Exemplo de `pipelines/hist_returns.py`

```python
def run(ref_date=None):
    AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING")
    logging.info("Running hist_returns.run()")
    lq = LuxorQuery(adls_connection_string=AZURE_STORAGE_CONNECTION_STRING)

    if ref_date is None:
        ref_date = dt.date.today()
        ref_date = dt.date(ref_date.year, ref_date.month, 1) - dt.timedelta(days=1)
    ref_date = lq.get_month_end(ref_date)

    logging.info(f"Rodando para {ref_date}")
    returns_data = get_returns(lq, ref_date)

    if returns_data is not None and not returns_data.empty:
        incremental_load(lq, 'hist_returns', returns_data, normalize_columns=True)
    else:
        logging.info(f"Nenhum dado gerado para {ref_date}.")
```

---

### Exemplo de `run_hist_returns/run_hist_returns.py`

```python
from pipelines import hist_returns

def run():
    hist_returns.run()
```

---

### Exemplo de `run_hist_returns/__init__.py`

```python
import logging
from dotenv import load_dotenv
from run_hist_returns.run_hist_returns import run

load_dotenv()

def main(mytimer):
    logging.info("Executando funÃ§Ã£o run_hist_returns")
    try:
        run()
        logging.info("ExecuÃ§Ã£o concluÃ­da com sucesso.")
    except Exception as e:
        logging.error(f"Erro: {e}")
        raise
```

---

### Exemplo de `run_hist_returns/functions.json`

```json
{
  "scriptFile": "__init__.py",
  "bindings": [
    {
      "name": "mytimer",
      "type": "timerTrigger",
      "direction": "in",
      "schedule": "0 30 18 * * 1-5"
    }
  ]
}
```

---

## ğŸ’¡ Boas PrÃ¡ticas

- Sempre manter a lÃ³gica de negÃ³cio em `pipelines/`, deixando o diretÃ³rio `run_...` apenas para orquestraÃ§Ã£o.
- Usar variÃ¡veis de ambiente para credenciais e configuraÃ§Ãµes.
- Garantir que cada `run()` seja **idempotente** para evitar duplicaÃ§Ã£o de dados.
- Configurar agendamentos (`schedule`) de acordo com a necessidade do negÃ³cio.
- Monitorar logs no Azure para identificar falhas rapidamente.

---
